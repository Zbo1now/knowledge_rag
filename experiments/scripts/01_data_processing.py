"""æ•°æ®æ¸…æ´—æµæ°´çº¿è„šæœ¬"""

import os
import re
import json
import hashlib
import uuid
import pdfplumber
import pandas as pd
from tqdm import tqdm

try:
    from docx import Document  # type: ignore[import-not-found]
except Exception:
    Document = None

# =================é…ç½®åŒºåŸŸ=================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
RAW_DATA_DIR = os.path.join(BASE_DIR, "data", "raw")
PROCESSED_DATA_DIR = os.path.join(BASE_DIR, "data", "processed")
OUTPUT_FILE = os.path.join(PROCESSED_DATA_DIR, "knowledge.jsonl")

# åˆ‡ç‰‡é…ç½®
CHUNK_SIZE = 500       # ç›®æ ‡å—å¤§å°
MIN_CHUNK_SIZE = 50    # ä¸¢å¼ƒå¤ªçŸ­çš„å—
OVERLAP_SENTENCES = 2  # é‡å å¥å­æ•°é‡ (è¯­ä¹‰é‡å )

# TODO ç‰©ç†åˆ‡å‰²å¤ªè¿‡è‰ç‡ï¼Œåç»­è¿›è¡Œä¿®æ”¹
# å»å™ªé…ç½®ï¼šé¡µé¢ä¸Šä¸‹è¾¹ç¼˜è£åˆ‡æ¯”ä¾‹ (å»é™¤é¡µçœ‰é¡µè„š)
TOP_CROP_RATIO = 0.05    # å»é™¤é¡¶éƒ¨ 5%
BOTTOM_CROP_RATIO = 0.08 # å»é™¤åº•éƒ¨ 8%
# =========================================

def generate_doc_id(file_name):
    """æ ¹æ®æ–‡ä»¶åç”Ÿæˆå”¯ä¸€æ–‡æ¡£ID (MD5)"""
    return hashlib.md5(file_name.encode('utf-8')).hexdigest()

def clean_text_basic(text):
    """åŸºç¡€æ¸…æ´—ï¼šå»å¤šä½™ç©ºæ ¼ï¼Œä½†ä¿ç•™å¥æ„"""
    if not text: return ""
    # 1. æ›¿æ¢è¿ç»­ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    # 2. å»é™¤æ§åˆ¶å­—ç¬¦
    text = "".join([c for c in text if c.isprintable()])
    return text.strip()

def split_sentences(text):
    """
    è¯­ä¹‰åˆ†å‰²ï¼šæŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·ã€æ¢è¡Œç¬¦åˆ‡åˆ†å¥å­
    ä¿ç•™æ ‡ç‚¹ç¬¦å·åœ¨å¥å­æœ«å°¾
    """
    # æ­£åˆ™é€»è¾‘ï¼šé‡åˆ° ã€‚ï¼ï¼Ÿ æˆ–è€… \n å°±åˆ‡åˆ†ï¼Œå¹¶ä¸”ä¿ç•™åˆ†éš”ç¬¦
    pattern = r'([ã€‚ï¼ï¼Ÿ\n])' 
    parts = re.split(pattern, text)
    sentences = []
    
    # å°†åˆ†å‰²çš„å†…å®¹é‡æ–°ç»„åˆï¼šå¥å­ + æ ‡ç‚¹
    current_sent = ""
    for part in parts:
        current_sent += part
        if re.match(pattern, part): # å¦‚æœæ˜¯æ ‡ç‚¹ï¼Œç»“æŸå½“å‰å¥
            sentences.append(current_sent.strip())
            current_sent = ""
            
    if current_sent: # å¤„ç†æœ€åå‰©ä½™éƒ¨åˆ†
        sentences.append(current_sent.strip())
        
    return [s for s in sentences if s]

def semantic_chunking(text, chunk_size=CHUNK_SIZE):
    """
    è¯­ä¹‰åˆ‡ç‰‡ï¼šåŸºäºå¥å­èšåˆï¼Œè€Œä¸æ˜¯å­—ç¬¦ç¡¬åˆ‡
    """
    sentences = split_sentences(text)
    chunks = []
    
    current_chunk = []
    current_len = 0
    
    for i, sent in enumerate(sentences):
        sent_len = len(sent)
        
        # å¦‚æœåŠ ä¸Šè¿™å¥è¯è¶…å‡ºäº†é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰å—
        if current_len + sent_len > chunk_size and current_len > 0:
            # 1. ä¿å­˜å½“å‰å—
            full_chunk_text = "".join(current_chunk)
            chunks.append(full_chunk_text)
            
            # 2. å¼€å¯æ–°å—ï¼Œå¹¶å›é€€é‡å  (Overlap)
            # å–æœ€å OVERLAP_SENTENCES å¥è¯ä½œä¸ºæ–°å—çš„å¼€å¤´
            overlap_data = current_chunk[-OVERLAP_SENTENCES:] if len(current_chunk) >= OVERLAP_SENTENCES else current_chunk
            current_chunk = list(overlap_data)
            current_len = sum(len(s) for s in overlap_data)
        
        # åŠ å…¥å½“å‰å¥
        current_chunk.append(sent)
        current_len += sent_len
        
    # å¤„ç†æœ€åä¸€å—
    if current_chunk:
        chunks.append("".join(current_chunk))
        
    return chunks

def extract_title(text):
    """
    ç®€å•å¯å‘å¼è§„åˆ™æå–æ ‡é¢˜ (ç”¨äºå¢å¼ºä¸Šä¸‹æ–‡)
    ä¾‹å¦‚ï¼š "1.1 å®‰å…¨é¡»çŸ¥" æˆ– "ç¬¬ä¸€ç«  æ€»åˆ™"
    """
    # å–æ–‡æœ¬çš„å‰50ä¸ªå­—æ£€æŸ¥
    head = text[:50]
    match = re.search(r'(^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« |^[\d\.]+\s)', head)
    if match:
        return match.group().strip()
    return ""

def process_pdf(file_path, file_name, doc_id):
    results = []
    current_section_title = "æœªçŸ¥ç« èŠ‚" # è®°å½•ä¸Šä¸‹æ–‡
    
    try:
        with pdfplumber.open(file_path) as pdf:
            for i, page in enumerate(tqdm(pdf.pages, desc=f"è§£æ {file_name}", leave=False)):
                page_num = i + 1
                
                # --- 1. å»å™ªï¼šç‰©ç†è£åˆ‡ (Crop) ---
                # è·å–é¡µé¢å°ºå¯¸
                width = page.width
                height = page.height
                
                # å®šä¹‰ä¿ç•™åŒºåŸŸ (å»é™¤é¡µçœ‰é¡µè„š)
                bbox = (
                    0,                      # x0
                    height * TOP_CROP_RATIO, # top
                    width,                  # x1
                    height * (1 - BOTTOM_CROP_RATIO) # bottom
                )
                
                cropped_page = page.crop(bbox)
                text = cropped_page.extract_text()
                
                if not text: continue
                
                # --- 2. æ ‡é¢˜è¯†åˆ« ---
                # å¦‚æœè¿™ä¸€é¡µå¼€å¤´åƒæ˜¯æ ‡é¢˜ï¼Œæ›´æ–°å½“å‰ç« èŠ‚
                possible_title = extract_title(text)
                if possible_title:
                    current_section_title = possible_title
                
                # --- 3. æ¸…æ´— ---
                cleaned_text = clean_text_basic(text)
                
                # --- 4. è¯­ä¹‰åˆ‡ç‰‡ ---
                chunks = semantic_chunking(cleaned_text)
                
                for idx, chunk_text in enumerate(chunks):
                    if len(chunk_text) < MIN_CHUNK_SIZE:
                        continue
                        
                    # --- 5. æ„å»ºä¸°å¯Œå…ƒæ•°æ® ---
                    # ç”Ÿæˆå”¯ä¸€ chunk_id
                    chunk_id = f"{doc_id}_{page_num}_{idx}"
                    
                    # æå–é”šç‚¹ (Anchor)ï¼šå–ç¬¬ä¸€å¥è¯ï¼Œç”¨äºå‰ç«¯é«˜äº®
                    # å¦‚æœæ‰¾ä¸åˆ°æ ‡ç‚¹ï¼Œå°±å–å‰30ä¸ªå­—
                    first_sent_match = re.match(r'[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]', chunk_text)
                    anchor = first_sent_match.group() if first_sent_match else chunk_text[:30]
                    
                    record = {
                        "doc_id": doc_id,
                        "chunk_id": chunk_id,
                        "title": file_name,
                        "source": file_name,
                        "file_type": "pdf",
                        "page_num": page_num,
                        "section_title": current_section_title, # ğŸ”¥ å¢åŠ ä¸Šä¸‹æ–‡
                        "content": chunk_text, # ğŸ”¥ æ¸…æ´—åçš„çº¯æ–‡æœ¬ (ç”¨äºæœç´¢)
                        "anchor_text": anchor, # ğŸ”¥ ç”¨äºå‰ç«¯é«˜äº®å®šä½
                        "chunk_len": len(chunk_text)
                    }
                    results.append(record)
                    
    except Exception as e:
        print(f"âŒ è§£æ PDF å‡ºé”™ {file_name}: {e}")
        
    return results

def process_excel(file_path, file_name, doc_id):
    # Excel å¤„ç†é€»è¾‘ä¿æŒä¸å˜ï¼Œå¢åŠ  ID ç”Ÿæˆå³å¯
    results = []
    try:
        if file_name.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            df = pd.read_excel(file_path)
        df = df.fillna('')
        
        for index, row in df.iterrows():
            row_num = index + 1
            # å°†æ‰€æœ‰åˆ—åˆå¹¶ï¼ŒåŠ ä¸Šåˆ—åä½œä¸ºä¸Šä¸‹æ–‡
            parts = []
            for col_name, val in row.items():
                if str(val).strip():
                    parts.append(f"{col_name}:{val}")
            
            row_content = " ".join(parts)
            cleaned_content = clean_text_basic(row_content)
            
            if not cleaned_content: continue
            
            chunk_id = f"{doc_id}_row_{row_num}"
            
            results.append({
                "doc_id": doc_id,
                "chunk_id": chunk_id,
                "title": file_name,
                "source": file_name,
                "file_type": "table",
                "page_num": row_num,
                "section_title": "æ•…éšœæ—¥å¿—è¡¨",
                "content": cleaned_content,
                "anchor_text": cleaned_content[:30],
                "chunk_len": len(cleaned_content)
            })
    except Exception as e:
        print(f"âŒ è§£æè¡¨æ ¼å‡ºé”™ {file_name}: {e}")
    return results


def process_txt(file_path, file_name, doc_id):
    results = []
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            raw_text = f.read()

        if not raw_text:
            return results

        # è§£æä¸‹è½½è„šæœ¬å¤´éƒ¨ï¼šæ ‡é¢˜è¡Œ + ===== åˆ†éš” + URL: ...
        raw_lines = [ln.rstrip() for ln in raw_text.splitlines()]
        lines = [ln.strip() for ln in raw_lines if ln.strip()]

        title = "æœªçŸ¥æ ‡é¢˜"
        source_url = ""

        if lines:
            # å…¼å®¹ï¼šç¬¬ä¸€è¡Œå°±æ˜¯æ ‡é¢˜ï¼ˆ02_download_details.py ä¼šå†™çœŸå®æ ‡é¢˜ï¼‰
            title = lines[0]
            if title.lower().startswith("title:"):
                title = title.split(":", 1)[1].strip() or title

            # å‰å‡ è¡Œé‡Œæ‰¾ URL
            for ln in lines[:12]:
                if ln.lower().startswith("url:"):
                    source_url = ln.split(":", 1)[1].strip()
                    break

        # è®¡ç®—æ­£æ–‡èµ·å§‹ä½ç½®ï¼šè·³è¿‡ titleã€====ã€URL è¡Œ
        body_lines: list[str] = []
        for idx, ln in enumerate(lines):
            if idx == 0:
                continue
            if set(ln) == {"="}:
                continue
            if ln.lower().startswith("url:"):
                continue
            body_lines.append(ln)

        body_text = "\n".join(body_lines) if body_lines else raw_text
        cleaned_text = clean_text_basic(body_text)
        if not cleaned_text:
            return results

        chunks = semantic_chunking(cleaned_text)
        for idx, chunk_text in enumerate(chunks):
            if len(chunk_text) < MIN_CHUNK_SIZE:
                continue

            chunk_id = f"{doc_id}_txt_{idx}"
            first_sent_match = re.match(r'[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]', chunk_text)
            anchor = first_sent_match.group() if first_sent_match else chunk_text[:30]

            results.append(
                {
                    "doc_id": doc_id,
                    "chunk_id": chunk_id,
                    "title": title,
                    "source_url": source_url,
                    "source": file_name,
                    "file_type": "txt",
                    "page_num": idx + 1,
                    "section_title": title[:50] if title else "æœªçŸ¥æ ‡é¢˜",
                    "content": chunk_text,
                    "anchor_text": anchor,
                    "chunk_len": len(chunk_text),
                }
            )

    except Exception as e:
        print(f"âŒ è§£æ TXT å‡ºé”™ {file_name}: {e}")

    return results


def process_docx(file_path, file_name, doc_id):
    results = []
    if Document is None:
        print(f"âš ï¸ è·³è¿‡ DOCXï¼ˆç¼ºå°‘ä¾èµ– python-docxï¼‰ï¼š{file_name}")
        return results

    try:
        doc = Document(file_path)
        text = "\n".join([p.text for p in doc.paragraphs if p.text and p.text.strip()])
        cleaned_text = clean_text_basic(text)
        if not cleaned_text:
            return results

        chunks = semantic_chunking(cleaned_text)
        for idx, chunk_text in enumerate(chunks):
            if len(chunk_text) < MIN_CHUNK_SIZE:
                continue

            chunk_id = f"{doc_id}_docx_{idx}"
            first_sent_match = re.match(r'[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]', chunk_text)
            anchor = first_sent_match.group() if first_sent_match else chunk_text[:30]

            results.append(
                {
                    "doc_id": doc_id,
                    "chunk_id": chunk_id,
                    "title": file_name,
                    "source": file_name,
                    "file_type": "docx",
                    "page_num": idx + 1,
                    "section_title": "Wordæ–‡æ¡£",
                    "content": chunk_text,
                    "anchor_text": anchor,
                    "chunk_len": len(chunk_text),
                }
            )
    except Exception as e:
        print(f"âŒ è§£æ DOCX å‡ºé”™ {file_name}: {e}")

    return results

def main():
    if not os.path.exists(PROCESSED_DATA_DIR):
        os.makedirs(PROCESSED_DATA_DIR)
        
    all_data = []
    
    print(f"ğŸš€ å¼€å§‹å¤„ç†ï¼Œå»å™ªç­–ç•¥ï¼šTop {TOP_CROP_RATIO*100}%, Bottom {BOTTOM_CROP_RATIO*100}%")
    
    for root, _, files in os.walk(RAW_DATA_DIR):
        for file_name in files:
            if file_name.startswith('.'): 
                continue

            file_path = os.path.join(root, file_name)
            if os.path.isdir(file_path):
                continue

            # ç”¨ç›¸å¯¹è·¯å¾„åš sourceï¼Œé¿å…åŒåæ–‡ä»¶å†²çª
            rel_path = os.path.relpath(file_path, RAW_DATA_DIR).replace("\\", "/")
            doc_id = generate_doc_id(rel_path)
            file_ext = file_name.lower().split('.')[-1]

            if file_ext == 'pdf':
                all_data.extend(process_pdf(file_path, rel_path, doc_id))
            elif file_ext in ['xlsx', 'xls', 'csv']:
                all_data.extend(process_excel(file_path, rel_path, doc_id))
            elif file_ext == 'txt':
                all_data.extend(process_txt(file_path, rel_path, doc_id))
            elif file_ext == 'docx':
                all_data.extend(process_docx(file_path, rel_path, doc_id))
            
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(all_data)} æ¡å¢å¼ºæ•°æ®...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for record in all_data:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
            
    print(f"âœ… å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()