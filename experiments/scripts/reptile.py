import time
import urllib.parse
import os

try:
    import pandas as pd  # type: ignore[import-not-found]
except ImportError as e:
    raise SystemExit("ç¼ºå°‘ä¾èµ– pandasï¼›è¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt") from e

try:
    import requests  # type: ignore[import-not-found]
except ImportError as e:
    raise SystemExit("ç¼ºå°‘ä¾èµ– requestsï¼›è¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt") from e

try:
    from bs4 import BeautifulSoup  # type: ignore[import-not-found]
except ImportError as e:
    raise SystemExit("ç¼ºå°‘ä¾èµ– beautifulsoup4ï¼›è¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt") from e

# ================= é…ç½®åŒºåŸŸ =================
BASE_DOMAIN = "https://zh-cn.cncmachiningptj.com"
START_PAGE = 9
# è®¾ä¸º None è¡¨ç¤ºä¸€ç›´çˆ¬åˆ° 404ï¼ˆæœ«é¡µï¼‰ï¼›ä¹Ÿå¯ä»¥å¡«æ•°å­—ï¼Œæ¯”å¦‚ 20
END_PAGE: int | None = None
SLEEP_SECONDS = 1.5

# ç½‘ç»œä¸é‡è¯•
CONNECT_TIMEOUT_SECONDS = 10
READ_TIMEOUT_SECONDS = 25
MAX_RETRIES_PER_PAGE = 3
BACKOFF_BASE_SECONDS = 2.0
MAX_CONSECUTIVE_FAILURES = 5

# ä»…ç­›é€‰ï¼šæ ‡é¢˜åŒ…å«â€œå‹é“¸â€æˆ–â€œé“¸é€ â€
KEYWORDS = [
    "å‹é“¸",
    "é“¸é€ ",
]

# è¾“å‡º Excelï¼ˆä¼šä¿ç•™æ‰€æœ‰æŠ“åˆ°çš„æ–‡ç« ï¼Œå¹¶æ ‡è®°æ˜¯å¦å‘½ä¸­å…³é”®è¯ï¼‰
OUTPUT_FILE = "foundry_articles.xlsx"
# ===========================================


def normalize_url(href: str) -> str:
    if not href:
        return ""
    return urllib.parse.urljoin(BASE_DOMAIN, href)


headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}


def fetch_html(session: "requests.Session", url: str) -> str | None:
    """æŠ“å–å•é¡µ HTMLï¼›é‡åˆ°è¶…æ—¶/çŸ­æš‚ç½‘ç»œé—®é¢˜ä¼šé‡è¯•ã€‚"""
    last_err: Exception | None = None
    for attempt in range(1, MAX_RETRIES_PER_PAGE + 1):
        try:
            resp = session.get(
                url,
                headers=headers,
                timeout=(CONNECT_TIMEOUT_SECONDS, READ_TIMEOUT_SECONDS),
            )

            # 404 ç”±è°ƒç”¨æ–¹åˆ¤æ–­æ˜¯å¦ç»“æŸ
            if resp.status_code == 404:
                return "__HTTP_404__"

            if resp.status_code != 200:
                raise requests.HTTPError(f"HTTP {resp.status_code}")

            resp.encoding = "utf-8"
            return resp.text
        except (requests.Timeout, requests.ConnectionError, requests.HTTPError) as e:
            last_err = e
            if attempt < MAX_RETRIES_PER_PAGE:
                sleep_s = BACKOFF_BASE_SECONDS ** (attempt - 1)
                print(f"è¯·æ±‚å¼‚å¸¸ï¼ˆç¬¬ {attempt}/{MAX_RETRIES_PER_PAGE} æ¬¡ï¼‰ï¼š{e}ï¼›{sleep_s:.1f}s åé‡è¯•")
                time.sleep(sleep_s)
            else:
                break
        except Exception as e:
            # å…¶å®ƒæœªçŸ¥å¼‚å¸¸ä¸ç›²ç›®é‡è¯•ï¼Œç›´æ¥æŠ›ç»™è°ƒç”¨æ–¹å¤„ç†
            raise e

    print(f"æœ¬é¡µé‡è¯•ä»å¤±è´¥ï¼š{last_err}")
    return None

page_index = START_PAGE
rows: list[dict] = []
seen_urls: set[str] = set()
consecutive_failures = 0

session = requests.Session()

while True:
    if END_PAGE is not None and page_index > END_PAGE:
        print(f"å·²çˆ¬å–åˆ°è®¾å®šç»“æŸé¡µ Blog-{END_PAGE}ï¼Œåœæ­¢çˆ¬å–ã€‚")
        break
    url = f"{BASE_DOMAIN}/Blog-{page_index}"
    print(f"æ­£åœ¨æŠ“å–: {url}")

    try:
        html = fetch_html(session, url)
        if html == "__HTTP_404__":
            print("åˆ°è¾¾æœ€åä¸€é¡µï¼Œåœæ­¢çˆ¬å–ã€‚")
            break
        if html is None:
            consecutive_failures += 1
            print(f"è¿ç»­å¤±è´¥æ¬¡æ•°: {consecutive_failures}/{MAX_CONSECUTIVE_FAILURES}")
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                print("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢çˆ¬å–ã€‚")
                break
            page_index += 1
            time.sleep(SLEEP_SECONDS)
            continue

        consecutive_failures = 0
        soup = BeautifulSoup(html, "html.parser")

        # æ ¹æ®æµ‹è¯•è„šæœ¬éªŒè¯è¿‡çš„ç»“æ„ï¼šdiv.media-body å†…æœ‰ h4.media-heading aï¼Œç®€ä»‹åœ¨ p.des
        article_items = soup.select("div.media-body")
        if not article_items:
            print("æœ¬é¡µæœªæ‰¾åˆ°æ–‡ç« å—ï¼ˆdiv.media-bodyï¼‰ï¼Œåœæ­¢çˆ¬å–ã€‚")
            break

        kws_lower = [k.lower() for k in KEYWORDS]
        page_rows = 0

        for item in article_items:
            title_tag = item.select_one("h4.media-heading a")
            desc_tag = item.select_one("p.des")

            if not title_tag:
                continue

            title = title_tag.get_text(strip=True)
            link = normalize_url(title_tag.get("href", ""))
            desc = desc_tag.get_text(strip=True) if desc_tag else ""

            if not link or link in seen_urls:
                continue

            title_lower = title.lower()
            is_relevant = any(k in title_lower for k in kws_lower)
            if not is_relevant:
                continue

            rows.append(
                {
                    "æ ‡é¢˜": title,
                    "é“¾æ¥": link,
                    "ç®€ä»‹": desc,
                    "é¡µç ": page_index,
                }
            )
            seen_urls.add(link)
            page_rows += 1

        print(f"æœ¬é¡µæå– {page_rows} æ¡ï¼ˆç´¯è®¡ {len(rows)} æ¡ï¼‰")

        page_index += 1
        time.sleep(SLEEP_SECONDS)

    except Exception as e:
        print(f"å‡ºé”™: {e}")
        break


if not rows:
    print("\nâš ï¸ æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚")
else:
    df = pd.DataFrame(rows)

    # åˆå¹¶å†å²æ•°æ®ï¼Œé¿å…è¦†ç›–
    if os.path.exists(OUTPUT_FILE):
        try:
            old_df = pd.read_excel(OUTPUT_FILE)
            df = pd.concat([old_df, df], ignore_index=True)
            if "é“¾æ¥" in df.columns:
                df = df.drop_duplicates(subset=["é“¾æ¥"], keep="first")
            print(f"æ£€æµ‹åˆ°å·²æœ‰ {OUTPUT_FILE}ï¼Œå·²åˆå¹¶å¹¶æŒ‰é“¾æ¥å»é‡ã€‚")
        except Exception as e:
            ts = time.strftime("%Y%m%d_%H%M%S")
            fallback = f"foundry_articles_{ts}.xlsx"
            print(f"è¯»å–æ—§ Excel å¤±è´¥ï¼ˆ{e}ï¼‰ï¼Œå°†å†™å…¥æ–°æ–‡ä»¶: {fallback}")
            OUTPUT_FILE = fallback

    df.to_excel(OUTPUT_FILE, index=False)

    print(f"\nâœ… å…±æ‰¾åˆ° {len(df)} æ¡æ ‡é¢˜åŒ…å«â€˜å‹é“¸/é“¸é€ â€™çš„æ–‡ç« ")
    print(f"ğŸ’¾ å·²ä¿å­˜: {OUTPUT_FILE}\n")

    for _, r in df.head(50).iterrows():
        print(f"ğŸ“Œ {r['æ ‡é¢˜']}\n   ğŸ”— {r['é“¾æ¥']}\n")