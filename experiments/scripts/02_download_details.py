import os
import time
import hashlib
import argparse
import json

try:
    import pandas as pd  # type: ignore[import-not-found]
except ImportError as e:
    raise SystemExit("ç¼ºå°‘ä¾èµ– pandasï¼›è¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt") from e

try:
    import requests  # type: ignore[import-not-found]
except ImportError as e:
    raise SystemExit("ç¼ºå°‘ä¾èµ– requestsï¼›è¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt") from e

try:
    from bs4 import BeautifulSoup  # type: ignore[import-not-found]
except ImportError as e:
    raise SystemExit("ç¼ºå°‘ä¾èµ– beautifulsoup4ï¼›è¯·å…ˆæ‰§è¡Œ: pip install -r requirements.txt") from e

from tqdm import tqdm
import re

# ================= é…ç½®åŒºåŸŸ =================
# 1. Excel æ–‡ä»¶è·¯å¾„
EXCEL_PATH = "foundry_articles.xlsx" 

# 2. ä¿å­˜ä½ç½®
SAVE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "data", "raw", "crawled_articles")

# 3. ğŸ”¥ æ ¹æ®æˆªå›¾ä¿®æ”¹ï¼šæ­£æ–‡é€‰æ‹©å™¨
CONTENT_SELECTOR = "div.met-editor" 

# ç½‘ç»œä¸é‡è¯•
CONNECT_TIMEOUT_SECONDS = 10
READ_TIMEOUT_SECONDS = 25
MAX_RETRIES_PER_ARTICLE = 3
BACKOFF_BASE_SECONDS = 2.0

# æ–‡ä»¶åç­–ç•¥ï¼šæ ‡é¢˜ + url hashï¼Œé¿å…åŒåè¦†ç›–
USE_URL_HASH_SUFFIX = True
# ===========================================

def clean_filename(title):
    name = re.sub(r'[\\/*?:"<>|]', "", str(title)).strip()
    return name if name else "untitled"


def extract_article_title(soup: "BeautifulSoup") -> str:
    # å°½é‡ä»æ­£æ–‡é¡µæå–â€œçœŸå®æ ‡é¢˜â€ï¼Œå¤±è´¥å†å›é€€åˆ° <title>
    candidates = [
        soup.select_one("h1"),
        soup.select_one("h1 a"),
        soup.select_one("h2"),
    ]
    for node in candidates:
        if node and node.get_text(strip=True):
            return node.get_text(strip=True)

    og = soup.select_one('meta[property="og:title"]')
    if og and og.get("content"):
        return str(og.get("content")).strip()

    if soup.title and soup.title.get_text(strip=True):
        title = soup.title.get_text(strip=True)
        # å¸¸è§çš„ç«™ç‚¹åç¼€æ¸…ç†ï¼ˆå°½é‡ä¿å®ˆï¼‰
        for sep in [" | ", " - "]:
            if sep in title:
                title = title.split(sep)[0].strip()
                break
        return title

    return ""


def table_to_markdown(table_tag: "BeautifulSoup") -> str:
    """æŠŠ HTML <table> è¿‘ä¼¼è½¬æ¢æˆ Markdown è¡¨æ ¼ï¼Œå°½é‡ä¿ç•™ç»“æ„ã€‚"""
    rows = []
    for tr in table_tag.select("tr"):
        cells = tr.find_all(["th", "td"])
        row = [c.get_text(" ", strip=True) for c in cells]
        if row and any(x for x in row):
            rows.append(row)

    if not rows:
        return ""

    # è¡¥é½åˆ—æ•°
    max_cols = max(len(r) for r in rows)
    rows = [r + [""] * (max_cols - len(r)) for r in rows]

    header = rows[0]
    sep = ["---"] * max_cols
    body = rows[1:]

    def fmt(r):
        return "| " + " | ".join((x or "").replace("\n", " ") for x in r) + " |"

    out = [fmt(header), fmt(sep)]
    out.extend(fmt(r) for r in body)
    return "\n".join(out)

def fetch_content(session: "requests.Session", url: str) -> tuple[str, str] | None:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36"
    }

    last_err: Exception | None = None
    for attempt in range(1, MAX_RETRIES_PER_ARTICLE + 1):
        try:
            resp = session.get(
                url,
                headers=headers,
                timeout=(CONNECT_TIMEOUT_SECONDS, READ_TIMEOUT_SECONDS),
            )
            if resp.status_code != 200:
                return None

            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            real_title = extract_article_title(soup)
        
            # ğŸ”¥ æ ¸å¿ƒé€»è¾‘ï¼šæå– met-editor ä¸‹çš„æ‰€æœ‰å†…å®¹
            content_div = soup.select_one(CONTENT_SELECTOR)
        
            if content_div:
                # å…ˆå°†æ­£æ–‡åŒºåŸŸå†…æ‰€æœ‰ table è½¬æ¢ä¸º Markdownï¼ˆé¿å…è¢« get_text æ‰“å¹³ï¼‰
                content_clone = BeautifulSoup(str(content_div), "html.parser")
                root_tag = content_clone.find(True)
                if root_tag is not None:
                    for t in list(root_tag.select("table")):
                        md = table_to_markdown(t)
                        if md:
                            t.replace_with(content_clone.new_string("\n" + md + "\n"))
                        else:
                            t.decompose()
                    content_div = root_tag

                # ğŸ’¡ ä¼˜åŒ–ï¼šä¸ç›´æ¥ get_textï¼Œè€Œæ˜¯æ‰‹åŠ¨éå†ï¼Œä¿ç•™æ ‡é¢˜çš„å±‚çº§æ„Ÿ
                lines = []
                for child in content_div.children:
                    if getattr(child, "name", None) is None:
                        text = str(child).strip()
                        if text:
                            lines.append(text)
                    elif child.name in ['h1', 'h2', 'h3']:
                        # ç»™å°æ ‡é¢˜åŠ ä¸ªæ ‡è®°ï¼Œæ¸…æ´—æ—¶ä¸€çœ‹å°±çŸ¥é“è¿™æ˜¯é‡ç‚¹
                        lines.append(f"\n### {child.get_text(strip=True)}\n")
                    elif child.name == 'p':
                        text = child.get_text(strip=True)
                        if text: # è·³è¿‡ç©ºæ®µè½
                            lines.append(text)
                    elif child.name == 'table':
                        md = table_to_markdown(child)
                        if md:
                            lines.append("\n" + md + "\n")
                    else:
                        # å…¶å®ƒæ ‡ç­¾å…œåº•æŠ½å–ï¼ˆtable å·²è¢«æ›¿æ¢ä¸º Markdown æ–‡æœ¬ï¼‰
                        text = child.get_text(separator="\n", strip=True)
                        if text:
                            lines.append(text)
            
                # å¦‚æœä¸Šé¢é‚£ç§ç²¾ç»†æå–æ²¡æ‹¿åˆ°ä¸œè¥¿ï¼ˆé˜²æ­¢ç½‘é¡µç»“æ„å¾®è°ƒï¼‰ï¼Œå°±å…œåº•ç”¨ get_text
                if not lines:
                    return real_title, content_div.get_text(separator="\n", strip=True)
                
                return real_title, "\n".join(lines)
            else:
                return real_title, ""

        except (requests.Timeout, requests.ConnectionError) as e:
            last_err = e
            if attempt < MAX_RETRIES_PER_ARTICLE:
                time.sleep(BACKOFF_BASE_SECONDS ** (attempt - 1))
                continue
            break
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥ {url}: {e}")
            return None

    if last_err is not None:
        print(f"æŠ“å–å¤±è´¥ {url}: {last_err}")
    return None

def main():
    parser = argparse.ArgumentParser(description="ä¸‹è½½æ–‡ç« æ­£æ–‡åˆ°æœ¬åœ° txtï¼ˆæ”¯æŒå•é“¾æ¥æµ‹è¯•ä¸æ‰¹é‡æ¨¡å¼ï¼‰")
    parser.add_argument("--url", type=str, default="", help="å•é“¾æ¥æµ‹è¯•ï¼šä¼ å…¥æ–‡ç« è¯¦æƒ…é¡µ URL")
    parser.add_argument("--out", type=str, default="", help="å•é“¾æ¥æµ‹è¯•ï¼šå¯é€‰è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤ä½¿ç”¨ æ ‡é¢˜+hashï¼‰")
    parser.add_argument("--no-save", action="store_true", help="å•é“¾æ¥æµ‹è¯•ï¼šåªæ‰“å°é¢„è§ˆï¼Œä¸å†™å…¥æ–‡ä»¶")
    parser.add_argument("--preview", type=int, default=400, help="å•é“¾æ¥æµ‹è¯•ï¼šæ­£æ–‡é¢„è§ˆå­—ç¬¦æ•°")
    parser.add_argument("--json", action="store_true", help="å•é“¾æ¥æµ‹è¯•ï¼šä»¥ JSON è¾“å‡ºï¼ˆtitle/url/contentï¼‰")
    parser.add_argument("--overwrite", action="store_true", help="æ‰¹é‡æ¨¡å¼ï¼šè¦†ç›–å·²å­˜åœ¨çš„ txtï¼ˆç”¨äºé‡æ–°ä¸‹è½½ä»¥æ›´æ–°è¡¨æ ¼/å†…å®¹ï¼‰")
    args = parser.parse_args()

    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    session = requests.Session()

    # ---- å•é“¾æ¥æµ‹è¯•æ¨¡å¼ ----
    if args.url and str(args.url).startswith("http"):
        link = str(args.url).strip()
        result = fetch_content(session, link)
        if result is None:
            print("âŒ å•é“¾æ¥æŠ“å–å¤±è´¥ã€‚")
            return

        real_title, content = result
        final_title = real_title.strip() if real_title and real_title.strip() else "untitled"

        if args.json:
            payload = {
                "title": final_title,
                "url": link,
                "content": content or "",
            }
            print(json.dumps(payload, ensure_ascii=False))
            return

        print(f"âœ… æ ‡é¢˜: {final_title}")
        print(f"âœ… URL: {link}")
        print("\n--- æ­£æ–‡é¢„è§ˆ ---")
        preview_n = max(0, int(args.preview))
        print((content or "")[:preview_n])
        print("\n--- é¢„è§ˆç»“æŸ ---\n")

        if args.no_save:
            return

        if args.out:
            file_name = args.out
            if not file_name.lower().endswith(".txt"):
                file_name += ".txt"
        else:
            safe_title = clean_filename(final_title)
            url_hash = hashlib.md5(link.encode("utf-8")).hexdigest()[:8]
            file_name = f"{safe_title}_{url_hash}.txt"

        save_path = os.path.join(SAVE_DIR, file_name)
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(f"{final_title}\n{'='*20}\n\n")
            f.write(f"URL: {link}\n\n")
            f.write(content or "")
        print(f"ğŸ’¾ å·²ä¿å­˜: {save_path}")
        return
        
    print(f"ğŸ“‚ æ­£åœ¨è¯»å– {EXCEL_PATH}...")
    try:
        df = pd.read_excel(EXCEL_PATH)
    except:
        df = pd.read_excel(os.path.join(os.getcwd(), EXCEL_PATH))

    print(f"å‘ç° {len(df)} ç¯‡æ–‡ç« ï¼Œå¼€å§‹æ‰¹é‡ä¸‹è½½...")
    
    success_count = 0
    
    # éå†ä¸‹è½½
    for index, row in tqdm(df.iterrows(), total=len(df), desc="ä¸‹è½½è¿›åº¦"):
        title = row['æ ‡é¢˜']
        link = row['é“¾æ¥']
        
        if not str(link).startswith("http"): continue
            
        safe_title = clean_filename(title)
        if USE_URL_HASH_SUFFIX:
            url_hash = hashlib.md5(str(link).encode("utf-8")).hexdigest()[:8]
            file_name = f"{safe_title}_{url_hash}.txt"
        else:
            file_name = f"{safe_title}.txt"

        save_path = os.path.join(SAVE_DIR, file_name)
        
        # æ–­ç‚¹ç»­ä¼ 
        if os.path.exists(save_path) and not args.overwrite:
            continue
            
        result = fetch_content(session, link)
        if result is None:
            continue

        real_title, content = result
        final_title = real_title.strip() if real_title and real_title.strip() else str(title).strip()
        
        if content and len(content) > 20:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(f"{final_title}\n{'='*20}\n\n") # ä½¿ç”¨æ­£æ–‡é¡µçœŸå®æ ‡é¢˜
                f.write(f"URL: {link}\n\n")
                f.write(content)
            success_count += 1
        
        time.sleep(0.3) # ç¨å¾®å¿«ä¸€ç‚¹ï¼Œ0.3ç§’ä¸€ç¯‡
        
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"å…±ä¿å­˜ {success_count} ç¯‡æ–‡æ¡£åˆ°: {SAVE_DIR}")

if __name__ == "__main__":
    main()